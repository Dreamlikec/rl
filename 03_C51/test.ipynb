{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 没有batch_size这个维度的前提下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, N, hidden=[128, 256]):\n",
    "        super(CDQN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.N = N\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], N*action_size)\n",
    "        self.output = nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(-1, self.action_size, self.N)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def new_distribution_checker(prev_distribution, reward, gamma):\n",
    "    new_distribution = np.zeros((1,N))\n",
    "    for j in range(N):\n",
    "        prob = prev_distribution[0,j] \n",
    "        new_val = vals[j] * gamma + reward\n",
    "        new_val = np.clip(new_val, v_min, v_max)\n",
    "        lower_index = np.floor((new_val - v_min)/unit).astype('int')\n",
    "        upper_index = np.minimum(lower_index + 1, N - 1)\n",
    "        lower_distance_ratio = 1 - (new_val - vals[lower_index])/unit\n",
    "        new_distribution[0,lower_index] += prob * lower_distance_ratio\n",
    "        new_distribution[0,upper_index] += prob * (1 - lower_distance_ratio)\n",
    "    return new_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_distribution(prev_distribution, reward, gamma):\n",
    "    new_vals = vals * gamma + reward\n",
    "    new_vals = np.clip(new_vals, v_min, v_max)\n",
    "    lower_indexes  = np.floor((new_vals - v_min)/unit).astype('int')\n",
    "    lower_distances = 1 - np.minimum((new_vals - vals[lower_indexes])/unit,1)\n",
    "    upper_indexes = np.minimum(lower_indexes + 1, N-1)\n",
    "    transition = np.zeros((N,N))\n",
    "    first_dim = range(N)\n",
    "    transition[first_dim, lower_indexes] += lower_distances\n",
    "    transition[first_dim, upper_indexes] += 1 - lower_distances\n",
    "    return prev_distribution.dot(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 51)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 51\n",
    "v_min = -5\n",
    "v_max = 5\n",
    "state_size = 8\n",
    "action_size = 4\n",
    "\n",
    "unit = (v_max - v_min)/(N - 1)\n",
    "vals = np.linspace(v_min,v_max,N)\n",
    "test_state = np.random.standard_normal((1,state_size))\n",
    "test_net = CDQN(state_size, action_size, N)\n",
    "with torch.no_grad():\n",
    "    result = test_net(torch.tensor(test_state,dtype = torch.float32)).exp()\n",
    "result = result[0, np.random.randint(0,action_size,1),:].numpy()\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reward = np.random.normal(1,1)\n",
    "test_gamma = 0.99\n",
    "new_dict = new_distribution(result, test_reward, test_gamma)\n",
    "new_dict_checker = new_distribution_checker(result, test_reward, test_gamma)\n",
    "\n",
    "(np.abs(new_dict - new_dict_checker) <= 0.001).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy implement: 0.0019927024841308594\n",
      "python implement: 0.043897151947021484\n",
      "numpy is 22.028954295285953 times to pthon implement\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "for _ in range(10):\n",
    "    new_distribution(result, test_reward, test_gamma)\n",
    "t2 = time.time() - t1\n",
    "print(\"numpy implement:\",t2)\n",
    "\n",
    "t1 = time.time()\n",
    "for _ in range(10):\n",
    "    new_distribution_checker(result, test_reward, test_gamma)\n",
    "t3 = time.time() - t1\n",
    "print(\"python implement:\",t3)\n",
    "print(\"numpy is {} times to pthon implement\".format(t3/t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有batch_size的前提下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_distribution_checker(prev_distribution, reward, gamma):\n",
    "    length = len(reward)\n",
    "    assert prev_distribution.shape[0] == length\n",
    "    if len(prev_distribution.shape) == 2:\n",
    "        prev_distribution = np.expand_dims(prev_distribution, 1)\n",
    "    new_distribution = np.zeros((length, 1, N))\n",
    "    for i in range(length):\n",
    "        for j in range(N):\n",
    "            prob = prev_distribution[i,0,j]\n",
    "            new_val = vals[j] * gamma + reward[i]\n",
    "            new_val = np.clip(new_val,v_min,v_max)\n",
    "            lower_index = np.floor((new_val - v_min)/unit).astype('int')\n",
    "            upper_index = np.minimum(lower_index+1,N-1)\n",
    "            lower_distance = 1 - (new_val - vals[lower_index])/unit\n",
    "            new_distribution[i,0,lower_index] += lower_distance*prob\n",
    "            new_distribution[i,0,upper_index] += (1-lower_distance)*prob\n",
    "    return new_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_distribution(prev_distribution, reward, gamma):\n",
    "    length = reward.shape[0]\n",
    "    assert prev_distribution.shape[0] == length\n",
    "    reward = reward.reshape(-1,1)\n",
    "    new_vals = vals.reshape(1,-1) * gamma + reward\n",
    "    new_vals = np.clip(new_vals,v_min,v_max)\n",
    "    lower_indexes = np.floor((new_vals-v_min)/unit).astype('int')\n",
    "    upper_indexes = np.minimum(lower_indexes+1,N-1)\n",
    "    lower_distance = 1 - np.minimum((new_vals - vals[lower_indexes])/unit,1)\n",
    "    transition = np.zeros((length,N,N))\n",
    "    first_dim = np.repeat(range(length),N)\n",
    "    second_dim = length * list(range(N)) \n",
    "    transition[first_dim, second_dim, lower_indexes.reshape(-1)] += lower_distance.reshape(-1)\n",
    "    transition[first_dim, second_dim, upper_indexes.reshape(-1)] += 1 - lower_distance.reshape(-1)\n",
    "    if len(prev_distribution.shape) == 2:\n",
    "        prev_distribution = np.expand_dims(prev_distribution,1) # (L,1,N)\n",
    "    return np.matmul(prev_distribution,transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 51\n",
    "v_min = -5\n",
    "v_max = 5\n",
    "state_size = 8\n",
    "action_size = 4\n",
    "batch_size = 128\n",
    "\n",
    "unit = (v_max - v_min)/(N - 1)\n",
    "vals = np.linspace(v_min,v_max,N)\n",
    "test_state = np.random.standard_normal((batch_size,1,state_size))\n",
    "test_net = CDQN(state_size, action_size, N)\n",
    "with torch.no_grad():\n",
    "    test_distribution = test_net(torch.tensor(test_state,dtype=torch.float32)).exp()\n",
    "test_distribution = test_distribution[range(batch_size),np.random.randint(0,action_size,batch_size),:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 51)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reward = np.random.uniform(-2,2,batch_size)\n",
    "test_gamma = 0.99\n",
    "new_dict = new_distribution(test_distribution, test_reward, test_gamma)\n",
    "new_dict_checker = new_distribution_checker(test_distribution, test_reward, test_gamma)\n",
    "# np.abs(new_dict - new_dict_checker)\n",
    "(np.abs(new_dict - new_dict_checker) <= 0.001).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy implement: 0.06682038307189941\n",
      "python implement: 2.231031894683838\n",
      "numpy is 33.38849303337912 times to pthon implement\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "for _ in range(10):\n",
    "    new_distribution(test_distribution, test_reward, test_gamma)\n",
    "t2 = time.time() - t1\n",
    "print(\"numpy implement:\",t2)\n",
    "\n",
    "t1 = time.time()\n",
    "for _ in range(10):\n",
    "    new_distribution_checker(test_distribution, test_reward, test_gamma)\n",
    "t3 = time.time() - t1\n",
    "print(\"python implement:\",t3)\n",
    "print(\"numpy is {} times to pthon implement\".format(t3/t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch实现batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_new_distribution(prev_distribution, reward, gamma):\n",
    "    vals_torch = torch.tensor(vals,dtype = torch.float32)\n",
    "    reward = reward.view(-1,1)\n",
    "    length = reward.size(0)\n",
    "    assert prev_distribution.shape[0] == length\n",
    "    new_vals = vals_torch.view(1,-1) * gamma + reward\n",
    "    new_vals = torch.clamp(new_vals,v_min,v_max)\n",
    "    lower_indexes = torch.floor((new_vals-v_min)/unit).long()\n",
    "    print(lower_indexes.shape)\n",
    "    upper_indexes = torch.min(lower_indexes+1,other = torch.tensor(N-1,dtype=torch.long))\n",
    "    lower_vals = vals_torch[lower_indexes]\n",
    "    print(lower_vals.shape)\n",
    "    lower_distance = 1 - torch.min((new_vals-lower_vals)/unit,other=torch.tensor(1,dtype=torch.float32))\n",
    "    transition = torch.zeros((length,N,N))\n",
    "    first_dim = torch.tensor(range(length),dtype=torch.long).view(-1,1).repeat(1,N).view(-1)\n",
    "    second_dim = torch.tensor(range(N),dtype=torch.long).repeat(length)\n",
    "    transition[first_dim, second_dim, lower_indexes.view(-1)] += lower_distance.view(-1)\n",
    "    transition[first_dim, second_dim, upper_indexes.view(-1)] += 1 - lower_distance.view(-1)\n",
    "    if len(prev_distribution.shape) == 2:\n",
    "        prev_distribution = prev_distribution.unsqueeze(1)\n",
    "    return torch.bmm(prev_distribution,transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 51])\n",
      "torch.Size([128, 51])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_distribution_torch = torch.tensor(test_distribution, dtype=torch.float32)\n",
    "test_reward_torch = torch.tensor(test_reward, dtype=torch.float32)\n",
    "\n",
    "new_dist_torch = torch_new_distribution(test_distribution_torch, test_reward_torch, test_gamma)\n",
    "(np.abs(new_dist_torch.numpy() - new_dict)<=0.0001).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025931596755981445\n",
      "0.06382966041564941\n"
     ]
    }
   ],
   "source": [
    "test_distribution_torch = torch.tensor(test_distribution, dtype=torch.float32)\n",
    "test_reward_torch = torch.tensor(test_reward, dtype=torch.float32)\n",
    "\n",
    "t1 = time.time()\n",
    "for _ in range(10):\n",
    "    torch_new_distribution(test_distribution_torch, test_reward_torch, test_gamma)\n",
    "print(time.time() - t1)\n",
    "\n",
    "t1 = time.time()\n",
    "for _ in range(10):\n",
    "    new_distribution(test_distribution, test_reward, test_gamma)\n",
    "print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 51])\n",
      "torch.Size([128, 4, 51])\n",
      "torch.Size([128, 1, 51])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "value = torch.randn(128,4)\n",
    "\n",
    "value,indexs = torch.max(input = value,dim = 1,keepdims = True)\n",
    "indexs = indexs.unsqueeze(1).repeat(1,1,51)\n",
    "\n",
    "distribution = torch.randn(128,4,51)\n",
    "print(indexs.shape)\n",
    "print(distribution.shape)\n",
    "\n",
    "new_prob = torch.gather(input = distribution, dim = 1, index = indexs)\n",
    "print(new_prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0751,  0.3832,  1.0936,  ..., -0.8018,  0.2763,  0.1824],\n",
       "         [ 0.6037,  0.6597,  0.6175,  ...,  0.0908,  0.6352,  0.8452],\n",
       "         [ 0.9136, -0.4437,  2.5809,  ...,  0.1672, -1.0536,  0.1368],\n",
       "         [ 0.6401, -0.7442, -0.1780,  ...,  0.3915, -0.0816,  0.1285]],\n",
       "\n",
       "        [[-0.6981,  2.5210,  2.0635,  ...,  1.7243, -0.0054, -0.0454],\n",
       "         [-0.3141, -1.2203, -0.9132,  ...,  0.4656, -0.2350, -0.7108],\n",
       "         [ 0.4908,  0.6577,  1.2031,  ..., -0.1226,  0.9048,  0.6337],\n",
       "         [-0.6079,  0.0950, -0.3259,  ...,  0.0594,  0.7983,  0.5678]],\n",
       "\n",
       "        [[ 0.2770, -0.3961,  0.1367,  ...,  0.8476, -1.0872, -1.0798],\n",
       "         [ 1.9798, -0.8580, -0.3838,  ...,  0.0803, -1.6462,  0.1190],\n",
       "         [ 0.1148, -0.0337,  0.2880,  ..., -1.5193, -0.2405,  0.0761],\n",
       "         [-1.3058,  0.6355, -0.2018,  ...,  1.4698, -1.1038, -0.5393]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.6385,  1.3177,  0.5646,  ...,  0.8797, -1.0995, -0.4916],\n",
       "         [-3.2929,  1.5723, -1.0038,  ..., -0.4519,  0.9137, -0.7705],\n",
       "         [ 1.1086, -0.4337, -0.7133,  ..., -0.1657, -1.9752,  0.1250],\n",
       "         [ 0.4332,  1.2287, -0.2248,  ..., -0.2445,  0.2774,  0.2009]],\n",
       "\n",
       "        [[-0.5071, -0.7824,  0.2119,  ..., -1.3121,  0.7589, -2.3495],\n",
       "         [-0.0167,  1.5197,  1.2674,  ..., -0.3020,  1.0648, -0.0976],\n",
       "         [-0.1254, -1.3787, -0.1261,  ...,  1.6945, -0.6220, -1.8914],\n",
       "         [-1.1284,  1.1589, -1.3505,  ..., -0.6170, -1.5899,  0.0737]],\n",
       "\n",
       "        [[-0.7166,  0.4359, -0.8712,  ..., -0.5412, -1.9588,  1.2418],\n",
       "         [-0.8547,  0.5395,  0.4790,  ...,  0.6904,  0.7141,  1.9098],\n",
       "         [ 0.6765,  0.0151,  0.2641,  ...,  0.3076, -0.2003, -1.3691],\n",
       "         [ 1.1351, -2.3543,  0.0219,  ..., -0.2704, -0.3111,  0.7695]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  7.0875],\n",
       "        [  6.8500],\n",
       "        [  5.7859],\n",
       "        [-13.0540],\n",
       "        [ -9.4969],\n",
       "        [ -2.0813],\n",
       "        [ -2.1127],\n",
       "        [ -2.5662],\n",
       "        [  2.4416],\n",
       "        [  2.5607],\n",
       "        [ -2.7764],\n",
       "        [  1.8979],\n",
       "        [ -1.3486],\n",
       "        [  9.1250],\n",
       "        [ -6.7386],\n",
       "        [  3.0038],\n",
       "        [  0.8740],\n",
       "        [  5.1195],\n",
       "        [ 14.9664],\n",
       "        [ 10.9062],\n",
       "        [  3.1666],\n",
       "        [ -2.8788],\n",
       "        [ -3.5492],\n",
       "        [ -0.8710],\n",
       "        [ 12.6076],\n",
       "        [ 11.8837],\n",
       "        [  0.5997],\n",
       "        [  5.6149],\n",
       "        [  0.4704],\n",
       "        [  0.8800],\n",
       "        [  1.1790],\n",
       "        [ -2.8200],\n",
       "        [  1.2729],\n",
       "        [ -2.6118],\n",
       "        [-10.3240],\n",
       "        [ -0.9134],\n",
       "        [ -5.5520],\n",
       "        [  3.9151],\n",
       "        [  8.3325],\n",
       "        [  1.7525],\n",
       "        [ -1.4840],\n",
       "        [  3.0378],\n",
       "        [ -7.3753],\n",
       "        [ -5.2572],\n",
       "        [ -4.1449],\n",
       "        [ -2.4071],\n",
       "        [ -9.0305],\n",
       "        [  9.9860],\n",
       "        [  4.0289],\n",
       "        [ -9.2216],\n",
       "        [ -0.6555],\n",
       "        [  6.3569],\n",
       "        [ -8.2446],\n",
       "        [ 22.3894],\n",
       "        [ -8.0120],\n",
       "        [  3.7720],\n",
       "        [  5.4797],\n",
       "        [  0.4713],\n",
       "        [ -7.8746],\n",
       "        [  8.3542],\n",
       "        [  8.1833],\n",
       "        [  1.5199],\n",
       "        [  5.5146],\n",
       "        [  2.4883],\n",
       "        [ -7.1725],\n",
       "        [  7.5755],\n",
       "        [ -7.4808],\n",
       "        [ -8.1184],\n",
       "        [  6.2619],\n",
       "        [ -5.1948],\n",
       "        [ -4.8674],\n",
       "        [  2.6030],\n",
       "        [ 16.2787],\n",
       "        [  8.4545],\n",
       "        [  1.0721],\n",
       "        [  3.5613],\n",
       "        [  6.7506],\n",
       "        [  4.1516],\n",
       "        [ -6.1496],\n",
       "        [  3.6339],\n",
       "        [-17.1125],\n",
       "        [  3.2594],\n",
       "        [-10.5762],\n",
       "        [ 16.5769],\n",
       "        [  0.2056],\n",
       "        [ -2.3539],\n",
       "        [  7.5004],\n",
       "        [  1.9394],\n",
       "        [ -2.1747],\n",
       "        [ -7.4646],\n",
       "        [ -2.3328],\n",
       "        [ -0.2932],\n",
       "        [ 12.6421],\n",
       "        [ -8.4201],\n",
       "        [ -3.0749],\n",
       "        [ -1.3149],\n",
       "        [ -4.9030],\n",
       "        [ -6.1540],\n",
       "        [  1.5642],\n",
       "        [ 14.2460],\n",
       "        [  6.0921],\n",
       "        [  5.6867],\n",
       "        [-23.3746],\n",
       "        [ -0.0280],\n",
       "        [  2.0020],\n",
       "        [ -5.3266],\n",
       "        [  3.9869],\n",
       "        [-11.6260],\n",
       "        [ 13.1907],\n",
       "        [  0.7863],\n",
       "        [ -1.4100],\n",
       "        [-11.2845],\n",
       "        [ -6.1792],\n",
       "        [  1.0609],\n",
       "        [ -2.6636],\n",
       "        [  9.3360],\n",
       "        [ -0.6288],\n",
       "        [ -3.3801],\n",
       "        [  2.4930],\n",
       "        [ -1.1644],\n",
       "        [  2.3468],\n",
       "        [  1.8403],\n",
       "        [-11.1825],\n",
       "        [ -2.6668],\n",
       "        [ -3.4229],\n",
       "        [ -2.9594],\n",
       "        [ -6.6375],\n",
       "        [ 17.4555]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.sum(axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4, 51])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.linspace(-5,5,51).view(1,1,51)\n",
    "\n",
    "value = torch.randn(128,4,51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.0000, -4.8000, -4.6000, -4.4000, -4.2000, -4.0000, -3.8000, -3.6000,\n",
       "        -3.4000, -3.2000, -3.0000, -2.8000, -2.6000, -2.4000, -2.2000, -2.0000,\n",
       "        -1.8000, -1.6000, -1.4000, -1.2000, -1.0000, -0.8000, -0.6000, -0.4000,\n",
       "        -0.2000,  0.0000,  0.2000,  0.4000,  0.6000,  0.8000,  1.0000,  1.2000,\n",
       "         1.4000,  1.6000,  1.8000,  2.0000,  2.2000,  2.4000,  2.6000,  2.8000,\n",
       "         3.0000,  3.2000,  3.4000,  3.6000,  3.8000,  4.0000,  4.2000,  4.4000,\n",
       "         4.6000,  4.8000,  5.0000])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.linspace(-5,5,51)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0,   0,  ..., 127, 127, 127])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(range(128),dtype=torch.long).view(-1,1).repeat(1,N).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  ..., 48, 49, 50])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(range(51),dtype=torch.long).repeat(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.6059)\n",
      "tensor(12.9914)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(128,1,51)\n",
    "b = torch.rand(128,1,51)\n",
    "print((a * b).mean(dim=2,keepdims = False).sum())\n",
    "print((a * b).sum(dim=2,keepdims = False).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2304, 0.9909, 0.5938,  ..., 0.5209, 0.6763, 0.8405]],\n",
       "\n",
       "        [[0.2790, 0.2014, 0.4780,  ..., 0.3732, 0.3972, 0.0119]],\n",
       "\n",
       "        [[0.0977, 0.1232, 0.8942,  ..., 0.2844, 0.5890, 0.9955]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.3420, 0.0195, 0.0962,  ..., 0.0829, 0.0053, 0.2469]],\n",
       "\n",
       "        [[0.2213, 0.9321, 0.8601,  ..., 0.0686, 0.8871, 0.8576]],\n",
       "\n",
       "        [[0.0290, 0.3218, 0.6698,  ..., 0.0227, 0.9045, 0.9442]]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
