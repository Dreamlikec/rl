{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from train_ram import train\n",
    "from agent import Agent\n",
    "from config import DEVICE as device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study\n",
    "\n",
    "Ablation的本意是`切除`，Ablation Study就是通过控制变量法，去除掉系统的一部分后，观察系统的表现，来理解每个独立组成部分对于整个系统的重要性。这里，我们主要考察两个部分的重要性：\n",
    "  * Baseline\n",
    "  * Normalization\n",
    "\n",
    "我们将在其他条件不变的情况下研究：\n",
    "  * 同时去除Baseline和Normalization\n",
    "  * 只去除Normalization\n",
    "  * 只去除Baseline\n",
    "  * 同时使用Baseline和Normalization\n",
    "\n",
    "然后，通过他们的训练表现，来判断每个部分的意义\n",
    "\n",
    "### Remove Baseline and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2b3e4c635da6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.99\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshare\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_critic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'episodic reward'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'black'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Projects\\强化学习学员资料\\Implement\\05_REINFORCE\\train_ram.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(agent, env, n_episode, max_t, scale)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mstate_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscounted_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Projects\\强化学习学员资料\\Implement\\05_REINFORCE\\agent.py\u001b[0m in \u001b[0;36mprocess_data\u001b[1;34m(self, states, actions, rewards, dones, batch_size)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshare\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mActor_critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mActor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, share=True, use_critic=False, normalize=False)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1500, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only remove Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, share=True, use_critic=True, normalize=False)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only remove Baseline (Still have implicit Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, share=True, use_critic=False, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply both Baseline and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, share=True, use_critic=True, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Learning Rate\n",
    "接下来，我们在小范围内调整学习率，来查看整个算法对学习率的敏感程度。值得一提的是，大部分的强化学习算法对于学习率都很敏感\n",
    "\n",
    "### lr-0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.001, gamma=0.99, device=device, share=True, use_critic=True, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.01, gamma=0.99, device=device, share=True, use_critic=True, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lr=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.05, gamma=0.99, device=device, share=True, use_critic=True, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Structure\n",
    "\n",
    "在介绍网络结构时我们说过，如果Actor和Critic有相同的前几层，可以认为前几层在同时学习两个task，主流的观念认为multi-task learning对于神经网络的训练整体是有帮助的，这里我们也给大家展示Actor和Critic完全分开的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, share=False, use_critic=True, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, share=False, use_critic=True, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Reward Scale\n",
    "很多时候，Reward的大小和范围会对训练算法有影响，在之前的实验中我们将Reward放缩为了原来的0.01倍，下面我们使用不同的放缩率，再看算法的表现情况\n",
    "\n",
    "### scale=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, share=True, use_critic=True, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.1)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scale=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, share=True, use_critic=True, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=1)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scale=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, share=True, use_critic=True, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.001)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.001))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Learning Frequency\n",
    "\n",
    "然后我们查看如果利用更多样本来学习是否会提升效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, share=True, use_critic=True, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1500, update_frequency=2, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, share=True, use_critic=True, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1500, update_frequency=4, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC or TD?\n",
    "\n",
    "最后，我们将计算G的方式从Monte Carlo切换为Temporal Difference，查看算法的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, mode='TD', share=False, use_critic=True, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, mode='TD', share=True, use_critic=True, normalize=False)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, mode='TD', share=False, use_critic=False, normalize=True)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, action_size, lr=0.005, gamma=0.99, device=device, mode='TD', share=False, use_critic=False, normalize=False)\n",
    "rewards, average_log = train(agent, env, n_episode=2000, max_t=1000, scale=0.01)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(rewards, label='episodic reward', color='black')\n",
    "plt.plot(average_log, label='moving average', color='green')\n",
    "plt.title('Share={}, use_critic={}, normalize={}, lr={}, Scale={}'.format(agent.share, agent.use_critic, agent.normalize, agent.lr, 0.01))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
